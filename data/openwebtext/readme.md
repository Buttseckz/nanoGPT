## Conjunto de dados OpenWebText

Depois de executar o arquivo `prepare.py` (pré-processamento), temos:

- train.bin tem aproximadamente 17GB, val.bin tem aproximadamente 8,5 MB
- train tem aproximadamente 9 bilhões de tokens (9,035,582,198)
- val tem aproximadamente 4 milhões de tokens (4,434,897)

Isso veio de um total de 8,013,769 documentos.

Referências:

- O conjunto de dados WebText da OpenAI é discutido no artigo [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- O conjunto de dados [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)
